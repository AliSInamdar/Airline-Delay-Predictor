# -*- coding: utf-8 -*-
"""Group__5_SD_Project_Phase2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ty3bBjsENkRjNgzsmww8gUqTW-p-qS50

## Scalable Database - CS673

## Group 5 - Project Phase 2

- Ali
- Likhitha
- Krishna
- Shivam
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

import joblib
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score
from sklearn.model_selection import cross_val_score
from sklearn.metrics import classification_report
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import train_test_split, GridSearchCV


# Suppress warnings
import warnings
warnings.filterwarnings('ignore')

df = pd.read_csv('/content/merged_data.csv')

df.dtypes

df.isnull().sum()

#Droping columns with a large number of missing values
df.drop(columns=['carrierdelay', 'weatherdelay', 'nasdelay', 'securitydelay', 'lateaircraftdelay','cancellationcode', 'arrdelay', 'depdelay'], inplace=True)
#To Check if there are any remaining missing values
print(df.isnull().sum())

from sklearn.impute import SimpleImputer

# Separate numerical and categorical columns
numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns
categorical_cols = df.select_dtypes(include=['object']).columns

# Impute missing values for numerical columns with the median
numerical_imputer = SimpleImputer(strategy='median')
df[numerical_cols] = numerical_imputer.fit_transform(df[numerical_cols])

# Impute missing values for categorical columns with the most frequent category
categorical_imputer = SimpleImputer(strategy='most_frequent')
df[categorical_cols] = categorical_imputer.fit_transform(df[categorical_cols])

# Check if there are any remaining missing values
print(df.isnull().sum())

df_encoded = df.copy()

label_encoder = LabelEncoder()
for column in df_encoded.columns:
    if df_encoded[column].dtype == 'object':  #To Check if the column is categorical
        df_encoded[column] = label_encoder.fit_transform(df_encoded[column])

df_encoded

df_encoded.dtypes

df_encoded.isnull().sum()

#numerical_cols = ['deptime', 'arrtime', 'actualelapsedtime', 'crselapsedtime', 'airtime', 'arrdelay', 'depdelay']
#for col in numerical_cols:
    #df_encoded[col].fillna(df_encoded[col].median(), inplace=True)

df_encoded

#Spliting the dataset into Training and Testing sets
X = df_encoded.drop(columns=['delayed'])
y = df_encoded['delayed']

# split the data in train validate test in 70:15:15

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=123)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=123)

import xgboost as xgb
# Split the data into training and testing sets
#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the XGBoost model
model = xgb.XGBClassifier(
    learning_rate=0.01,
    max_depth=15,
    n_estimators=1500,
    subsample=0.8,
    colsample_bytree=0.8,
    reg_alpha=0.1,
    reg_lambda=0.1,
    gamma=0.1,
    objective='binary:logistic'
)

model.fit(X_train, y_train)

# Make predictions on the test set
predictions = model.predict(X_test)

# Calculate accuracy score
accuracy = accuracy_score(y_test, predictions)
print("Accuracy Score:", accuracy)

# Calculate precision score
precision = precision_score(y_test, predictions)
print("Precision Score:", precision)

# Generate confusion matrix
conf_matrix = confusion_matrix(y_test, predictions)
print("Confusion Matrix:")
print(conf_matrix)

# Generate classification report
class_report = classification_report(y_test, predictions)
print("Classification Report:")
print(class_report)

# Initialize the decision tree classifier
decision_tree = DecisionTreeClassifier(random_state=123)

# Train the decision tree classifier on the training set
decision_tree.fit(X_train, y_train)

# Make predictions on the validation set
y_pred_val = decision_tree.predict(X_val)

# Calculate the accuracy score for the validation set
accuracy_val = accuracy_score(y_val, y_pred_val)
print("Validation Accuracy:", accuracy_val)

# Fine-tune the decision tree classifier using hyperparameter tuning
param_grid = {
    'max_depth': [None, 5, 10],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

grid_search = GridSearchCV(decision_tree, param_grid, cv=3)
grid_search.fit(X_train, y_train)

# Make predictions on the validation set using the best model
y_pred_val = grid_search.best_estimator_.predict(X_val)

# Calculate the accuracy score for the validation set using the best model
accuracy_val = accuracy_score(y_val, y_pred_val)
print("Validation Accuracy after hyperparameter tuning:", accuracy_val)

# Calculate the precision score for the validation set using the best model
precision_val = precision_score(y_val, y_pred_val)
print("Validation Precision:", precision_val)


# Make predictions on the test set using the best model
y_pred_test = grid_search.best_estimator_.predict(X_test)

# Calculate the accuracy score for the test set using the best model
accuracy_test = accuracy_score(y_test, y_pred_test)
print("Test Accuracy after hyperparameter tuning:", accuracy_test)

# Calculate the precision score for the test set using the best model
precision_test = precision_score(y_test, y_pred_test)
print("Test Precision:", precision_test)

# Generate confusion matrix for test set
conf_matrix_test = confusion_matrix(y_test, y_pred_test)
print("Confusion Matrix for Test Set:")
print(conf_matrix_test)

# Generate classification report for test set
class_report_test = classification_report(y_test, y_pred_test)
print("Classification Report for Test Set:")
print(class_report_test)

random_forest = RandomForestClassifier(
    n_estimators=200,       # Number of trees in the forest
    max_depth=10,            # Maximum depth of the tree
    min_samples_split=4,    # Minimum number of samples required to split an internal node
    min_samples_leaf=2,     # Minimum number of samples required to be at a leaf node
    random_state=84         # Random state for reproducibility
)

# Train the classifier
random_forest.fit(X_train, y_train)

# Make predictions on the validation set
predictions_val = random_forest.predict(X_val)

# Calculate accuracy on the validation set
accuracy_val = accuracy_score(y_val, predictions_val)
print("Validation Accuracy:", accuracy_val)

# Make predictions on the testing set
predictions_test = random_forest.predict(X_test)

# Calculate accuracy on the testing set
accuracy_test = accuracy_score(y_test, predictions_test)
print("Test Accuracy:", accuracy_test)

# Generate and print classification report
print("Classification Report:")
print(classification_report(y_test, predictions_test))

# Generate and print confusion matrix
print("Confusion Matrix:")
print(confusion_matrix(y_test, predictions_test))

# Initialize the Logistic Regression model with specified parameters
logistic_regression = LogisticRegression(
    C=2.0,                   # Inverse of regularization strength
    penalty='l2',            # Regularization penalty ('l1' or 'l2')
    solver='newton-cg',      # Algorithm to use in the optimization problem
    max_iter=200,            # Maximum number of iterations
    random_state=123          # Random state for reproducibility
)

# Train the model on the training set
logistic_regression.fit(X_train, y_train)

# Make predictions on the validation set
y_pred_val = logistic_regression.predict(X_val)

# Calculate the accuracy score for the validation set
accuracy_val = accuracy_score(y_val, y_pred_val)
print("Validation Accuracy:", accuracy_val)

# Calculate the precision score for the validation set
precision_val = precision_score(y_val, y_pred_val)
print("Validation Precision:", precision_val)

# Make predictions on the test set
y_pred_test = logistic_regression.predict(X_test)

# Calculate the accuracy score for the test set
accuracy_test = accuracy_score(y_test, y_pred_test)
print("Test Accuracy:", accuracy_test)

# Calculate the precision score for the test set
precision_test = precision_score(y_test, y_pred_test)
print("Test Precision:", precision_test)

# Generate confusion matrix for test set
conf_matrix_test = confusion_matrix(y_test, y_pred_test)
print("Confusion Matrix for Test Set:")
print(conf_matrix_test)

# Generate classification report for test set
class_report_test = classification_report(y_test, y_pred_test)
print("Classification Report for Test Set:")
print(class_report_test)

"""### Target Data Prediction"""

data = pd.read_csv('Target_data.csv')

data

data.info()

data.columns = [
    'year', 'month', 'dayofmonth', 'dayofweek', 'deptime', 'crsdeptime', 'arrtime', 'crsarrtime',
    'uniquecarrier', 'flightnum', 'tailnum', 'actualelapsedtime', 'crselapsedtime', 'airtime',
    'arrdelay', 'depdelay', 'origin', 'dest', 'distance', 'taxiin', 'taxiout', 'cancelled',
    'cancellationcode', 'diverted', 'carrierdelay', 'weatherdelay', 'nasdelay', 'securitydelay',
    'lateaircraftdelay', 'delayed'
]

#Droping columns with a large number of missing values
data.drop(columns=['carrierdelay', 'weatherdelay', 'nasdelay', 'securitydelay', 'lateaircraftdelay','cancellationcode', 'arrdelay', 'depdelay'], inplace=True)

# Separate numerical and categorical columns
numerical_cols = data.select_dtypes(include=['float64', 'int64']).columns
categorical_cols = data.select_dtypes(include=['object']).columns

# Impute missing values for numerical columns with the median
numerical_imputer = SimpleImputer(strategy='median')
data[numerical_cols] = numerical_imputer.fit_transform(data[numerical_cols])

# Impute missing values for categorical columns with the most frequent category
categorical_imputer = SimpleImputer(strategy='most_frequent')
data[categorical_cols] = categorical_imputer.fit_transform(data[categorical_cols])

# Check if there are any remaining missing values
print(data.isnull().sum())

df_encoded = data.copy()

label_encoder = LabelEncoder()
for column in df_encoded.columns:
    if df_encoded[column].dtype == 'object':  #To Check if the column is categorical
        df_encoded[column] = label_encoder.fit_transform(df_encoded[column])

df_encoded.dtypes

df_encoded

#numerical_cols = ['deptime', 'arrtime', 'actualelapsedtime', 'crselapsedtime', 'airtime', 'arrdelay', 'depdelay']
#for col in numerical_cols:
    #df_encoded[col].fillna(df_encoded[col].median(), inplace=True)

print(df_encoded.isnull().sum())

#Spliting the dataset into Training and Testing sets
X = df_encoded.drop(columns=['delayed'])
y = df_encoded['delayed']

# Save the second trained model to a file
joblib.dump(model, 'second_xgboost_model.pkl')

print("Second model saved successfully.")

loaded_model = joblib.load('second_xgboost_model.pkl')
predictions = loaded_model.predict(X)
# Create a DataFrame with original features and predictions
predictions_df_1 = pd.DataFrame(X, columns=X.columns)
predictions_df_1['predicted_delayed'] = predictions
predictions_df_1

predictions_df_1['predicted_delayed'] = predictions_df_1['predicted_delayed'].map({1: 'Y', 0: 'N'})

path = 'C:\\Users\\shiva\\Downloads\\'
csv_file = path + 'Predicted_flightsdelay.csv'
predictions_df_1.to_csv(csv_file, index=False)

predictions_df_1

predictions_df = predictions_df_1.copy()
predictions_df['depdelay'] = predictions_df['deptime'] - predictions_df['crsdeptime']
predictions_df['arrdelay'] = predictions_df['arrtime'] - predictions_df['crsarrtime']
predictions_df

# Select arrdelay, depdelay, and delayed columns
selected_columns = predictions_df.loc[:, ['arrdelay', 'depdelay', 'predicted_delayed']]

# Display the selected columns
selected_columns

selected_columns1 = predictions_df.loc[:, ['arrdelay', 'depdelay']]
selected_columns1

# Save the second trained model to a file
joblib.dump(gradient_boost, 'gradient_boost.pkl')

print("gradient_boost model saved successfully.")

loaded_model = joblib.load('gradient_boost.pkl')
predictions = loaded_model.predict(X)
# Create a DataFrame with original features and predictions
predictions_df_2 = pd.DataFrame(X, columns=X.columns)
predictions_df_2['predicted_delayed'] = predictions


predictions_df_2['predicted_delayed'] = predictions_df_2['predicted_delayed'].map({1: 'Y', 0: 'N'})

path = 'C:\\Users\\ali\\Downloads\\'
csv_file = path + 'Predicted_flightsdelay.csv'
predictions_df_2.to_csv(csv_file, index=False)

predictions_df_2

# Save the second trained model to a file
joblib.dump(decision_tree, 'decision_tree.pkl')

print("decision_tree model saved successfully.")

loaded_model = joblib.load('decision_tree.pkl')
predictions = loaded_model.predict(X)
# Create a DataFrame with original features and predictions
predictions_df_3 = pd.DataFrame(X, columns=X.columns)
predictions_df_3['predicted_delayed'] = predictions


predictions_df_3['predicted_delayed'] = predictions_df_3['predicted_delayed'].map({1: 'Y', 0: 'N'})

path = 'C:\\Users\\ali\\Downloads\\'
csv_file = path + 'Predicted_flightsdelay.csv'
predictions_df_3.to_csv(csv_file, index=False)

predictions_df_3

# Save the second trained model to a file
joblib.dump(random_forest, 'random_forest.pkl')

print("random_forest model saved successfully.")

loaded_model = joblib.load('random_forest.pkl')
predictions = loaded_model.predict(X)
# Create a DataFrame with original features and predictions
predictions_df_4 = pd.DataFrame(X, columns=X.columns)
predictions_df_4['predicted_delayed'] = predictions


predictions_df_4['predicted_delayed'] = predictions_df_4['predicted_delayed'].map({1: 'Y', 0: 'N'})

path = 'C:\\Users\\ali\\Downloads\\'
csv_file = path + 'Predicted_flightsdelay.csv'
predictions_df_4.to_csv(csv_file, index=False)

predictions_df_4

# Save the second trained model to a file
joblib.dump(logistic_regression, 'logistic_regression.pkl')

print("logistic_regression model saved successfully.")

loaded_model = joblib.load('logistic_regression.pkl')
predictions = loaded_model.predict(X)
# Create a DataFrame with original features and predictions
predictions_df_5 = pd.DataFrame(X, columns=X.columns)
predictions_df_5['predicted_delayed'] = predictions


predictions_df_5['predicted_delayed'] = predictions_df_5['predicted_delayed'].map({1: 'Y', 0: 'N'})

path = 'C:\\Users\\ali\\Downloads\\'
csv_file = path + 'Predicted_flightsdelay.csv'
predictions_df_5.to_csv(csv_file, index=False)

predictions_df_5

output_df_1 = pd.DataFrame(predictions_df_1['predicted_delayed'])
output_df_2 = pd.DataFrame(predictions_df_2['predicted_delayed'])
output_df_3 = pd.DataFrame(predictions_df_3['predicted_delayed'])
output_df_4 = pd.DataFrame(predictions_df_4['predicted_delayed'])
output_df_5 = pd.DataFrame(predictions_df_5['predicted_delayed'])

output_df = pd.concat([output_df_1, output_df_2, output_df_3, output_df_4, output_df_5], axis=1)

output_df.columns = ['XGBoost','GradientBoosting','DecisionTree','RandomForest','LogisticRegression']

output_df